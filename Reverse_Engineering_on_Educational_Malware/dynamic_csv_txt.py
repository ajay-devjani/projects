# Code Stats:
# Written by: Ajay Devjani, M.Tech INSE
# Project Co-partner: Rushaan Qureshi, M.Tech INSE
# Course: ENGR 6991: "5-Credits Project"
# Prof: Makan Pourzandi
# Concordia Session: Summer 2022

# Importing the libraries
import logging
import os

import pandas as pd


# Function definition to create input dataset from logs(CSVs) generated by Cuckoo Sandbox.
# Generated dataset is provided as input to all classifiers.
# Input: size: Specifies size of n-gram.
# Output: "training_dynamic_<<n-gram size>>_gram.txt" in "training" folder.
def dynamic_csv_to_txt(size=3):
    n_gram = []
    cwd = os.getcwd()
    # Take Cuckoo Benign Folder.
    i_path = os.path.join(cwd, "cuckoo_inputs/b_calls")
    # Iterate all files in specified directory.
    for filename in os.listdir(i_path):
        func_list = []
        # Import CSV
        data_frame = pd.read_csv(os.path.join(i_path, filename)).dropna()
        # Get all APIs of file into list.
        for index, row in data_frame.iterrows():
            func_list.append(row["api"])
        line = ''
        # Padding with __Nan__ if API calls sequence is less than n-gram size to maintain uniformity
        # in input CSV.
        if len(func_list) < size:
            for i in range(size - len(func_list)):
                line = line + "|__NaN__"
            n_gram.append(''.join(line).strip() + ",0")
            continue
        f = ''
        # Creating n-grams according to size. All API calls are separated by bar("|").
        for i in range(len(func_list)):
            if i == len(func_list) - size + 1:
                break
            for k in range(size - 1):
                f = f + func_list[i + k] + "|"
            f = f + func_list[i + k + 1] + ","
            n_gram.append(f + '0')
            f = ''

    # Take Cuckoo Malware Folder.
    i_path = os.path.join(cwd, "cuckoo_inputs/m_calls")
    # Iterate all files in specified directory.
    for filename in os.listdir(i_path):
        func_list = []
        # Import CSV
        data_frame = pd.read_csv(os.path.join(i_path, filename)).dropna()
        # Get all APIs of file into list.
        for index, row in data_frame.iterrows():
            func_list.append(row["api"])
        line = ''
        # Padding with __Nan__ if API calls sequence is less than n-gram size to maintain uniformity
        # in input CSV.
        if len(func_list) < size:
            for i in range(size - len(func_list)):
                line = line + "|__NaN__"
            n_gram.append(''.join(line).strip() + ",1")
            continue
        f = ''
        # Creating n-grams according to size. All API calls are separated by bar("|").
        for i in range(len(func_list)):
            if i == len(func_list) - size + 1:
                break
            for k in range(size - 1):
                f = f + func_list[i + k] + "|"
            f = f + func_list[i + k + 1] + ","
            n_gram.append(f + '1')
            f = ''

    # Writing the n-grams created to the "training_dynamic_<<n-gram size>>_gram.txt" in "training" folder.
    try:
        with open("./training/training_dynamic_" + str(size) + "_gram.txt", "w+", encoding="utf-8") as n_gram_writer:
            for line in n_gram:
                n_gram_writer.write(str(line) + '\n')
        n_gram_writer.close()
        print("Writing in training_dynamic_" + str(size) + "_gram.txt successful..")
        logging.info("Writing in training_dynamic_" + str(size) + "_gram.txt successful..")
    except Exception as ex:
        print("Error in writing training file." + str(ex))
        logging.error("Error in writing training file." + str(ex))
